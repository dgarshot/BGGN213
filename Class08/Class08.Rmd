---
title: "Class 08_PCA"
author: "Danielle Garshott"
date: "2/6/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### Clustering 

```{r}
# Generate some example data for clustering
tmp <- c(rnorm(30,-3), rnorm(30,3))
x <- cbind(x=tmp, y=rev(tmp))
plot(x)

# k means
# kmeans(x, center=3, nstart = 20)
# nstart (how many times dou you want to start this calculation over)
# Euclidean calculation d= sqr(x^2 + y^2 + z^2)

```
# tmp <- c(rnorm(30,-3), rnorm(30,3)) -> 30 points centered around -3, and 30 around 3 (60 points total)
# cbind -> putting the columns together

```{r}
km <- kmeans(x, 2, nstart = 20)
km
```

```{r}
km$size
```

```{r}
km$cluster
```

```{r}
plot(x, col=km$cluster)
points(km$centers, col="blue", pch=15, cex=2)
```


## Hierarchical clustering 

```{r}
# First we need to calculate point (dis)similarity
# as the Euclidean distance between observations
dist_matrix <- dist(x)
# The hclust() function returns a hierarchical
# clustering model
hc <- hclust(d = dist_matrix)
# the print method is not so useful here
hc 
```

```{r}
plot(hc)
abline(h=6, col="red")
grp2 <- cutree(hc, h=6)

```

```{r}
plot(x, col=grp2)
```

```{r}
plot(hc)
abline(h=2.5, col="blue")
grp6 <- cutree(hc, h=2.5)
table(grp6)
```

# We can also use k= groups as an argument to cutree! Instead of h= height

### Using different Linkage methods in R
```{r}
# Using different hierarchical clustering methods
hc.complete <- hclust(dist_matrix, method="complete")
plot(hc.complete)
hc.average <- hclust(dist_matrix, method="average")
plot(hc.average)
hc.single <- hclust(dist_matrix, method="single")
plot(hc.single)
```


# Made up overlapping data - a bit more real life like 
```{r}
# Step 1. Generate some example data for clustering
x <- rbind(
 matrix(rnorm(100, mean=0, sd = 0.3), ncol = 2), # c1
 matrix(rnorm(100, mean = 1, sd = 0.3), ncol = 2), # c2
 matrix(c(rnorm(50, mean = 1, sd = 0.3), # c3
 rnorm(50, mean = 0, sd = 0.3)), ncol = 2))
colnames(x) <- c("x", "y")
# Step 2. Plot the data without clustering
plot(x)
# Step 3. Generate colors for known clusters
# (just so we can compare to hclust results)
col <- as.factor( rep(c("c1","c2","c3"), each=50) )
plot(x, col=col)
```

```{r}
dist_realLife <- dist(x)

hc_realLife <- hclust(dist_realLife)
plot(hc_realLife)
 gp2 <- cutree(hc_realLife, k=2)
 plot(x, col=gp2)
 gp3 <- cutree(hc_realLife, k=3)
 plot(x, col=gp3)
 gp4 <- cutree(hc_realLife, k=9)
 plot(x, col=gp4)

```


### Principal Component Analysis (PCA)
# We have so many different points it compares all the data in a way that is more easily interpreted

```{r}
## You can also download this file from the class website!
mydata <- read.csv("https://tinyurl.com/expression-CSV",
 row.names=1)
# head command (first 6 lines of the data table)
head(mydata) 
```

**NOTE**: prcomp() expects the samples to be rows and
genes to be columns so we need to first transpose the
matrix with the t() function!

```{r}
## lets do PCA
# pca <- prcomp(t(mydata), scale=TRUE) 
# Take the transpose as the input data 

head( t(mydata))
```


```{r}
## lets do PCA
pca <- prcomp(t(mydata), scale=TRUE)
summary(pca)
```

# Make our first PCA plot
```{r}
## A basic PC1 vs PC2 2-D plot
dim(pca$x)
plot(pca$x[,1], pca$x[,2], xlab="PC1", ylab="PC2")

```

```{r}
## Variance captured per PC
pca.var <- pca$sdev^2 
# round command rounds up to a specified # of significant figures (i.e. 1 (shown))
round(pca.var/sum(pca.var)*100, 1) 
pca.var.per <- round(pca.var/sum(pca.var)*100, 1)
```


```{r}
barplot(pca.var.per, main="Scree Plot",
 xlab="Principal Component", ylab="Percent Variation")
```

# Make our PCA plot nice

```{r}
## A vector of colors for wt and ko samples
colvec <- as.factor( substr( colnames(mydata), 1, 2) )
plot(pca$x[,1], pca$x[,2], col=colvec, pch=16,
 xlab=paste0("PC1 (", pca.var.per[1], "%)"),
 ylab=paste0("PC2 (", pca.var.per[2], "%)")) 
```

```{r}
# Takes a string of characters or numerals and strings it out (i.e. 1,2 arguments call for position 1 and 2 of each colname)
substr( colnames(mydata), 1, 2)
```



### Principal Component Analysis (PCA)
# UK Foods

```{r}
x <- read.csv("UK_foods.csv")
head(x) 
```

```{r}
dim(x)
#17 rows, and 5 columns
```

```{r}
# Note how the minus indexing works
# Note how the minus indexing works: the -1 includes everything but the first column (this is what the neg sign does)
rownames(x) <- x[,1]
x <- x[,-1]
head(x)
```

```{r}
dim(x)
```

**Side-note**: An alternative approach to setting the correct row-names in this case would be to read the data filie again and this time set the row.names argument of read.csv() to be the first column (i.e. use argument setting row.names=1), see below:

# x <- read.csv("data/UK_foods.csv", row.names=1)

# Assign row names from the first col of the data upon reading. This is a safer approach. 
```{r}
x <- read.csv("UK_foods.csv", row.names=1)
head(x)
tail(x)
```

```{r}
# Generating barplots
# *Note* beside defaults to FALSE (which stacks bars)
barplot(as.matrix(x), beside=T, col=rainbow(nrow(x)))
```

```{r}
pairs(x, col=rainbow(10), pch=16)

# i.e. England is the y-axis for the first row, then each subsequent graph has the other countries on the x-axis
```

# PCA to the rescue!
```{r}
# Use the prcomp() PCA function (note: t is the transpose)
pca <- prcomp( t(x) )
summary(pca)
```


```{r}
# Plot PC1 vs PC2
plot(pca$x[,1], pca$x[,2], xlab="PC1", ylab="PC2", xlim=c(-270,500))
text(pca$x[,1], pca$x[,2], colnames(x), col=c("orange", "red", "blue", "darkgreen"))
```

# Loading values 
```{r}
## Lets focus on PC1 as it accounts for > 90% of variance 
par(mar=c(10, 3, 0.35, 0))
barplot( pca$rotation[,1], las=2 )
```






